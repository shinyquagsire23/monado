// Copyright 2021-2022, Collabora Ltd.
// Author: Jakob Bornecrantz <jakob@collabora.com>
// Author: Christoph Haag <christoph.haag@collabora.com>
// SPDX-License-Identifier: BSL-1.0

#version 460
#extension GL_GOOGLE_include_directive : require

#include "srgb.inc.glsl"

//! @todo should this be a spcialization const?
#define XRT_LAYER_STEREO_PROJECTION 0
#define XRT_LAYER_STEREO_PROJECTION_DEPTH 1
#define XRT_LAYER_QUAD 2
#define XRT_LAYER_CUBE 3
#define XRT_LAYER_CYLINDER 4
#define XRT_LAYER_EQUIRECT1 5
#define XRT_LAYER_EQUIRECT2 6

// Should we do timewarp.
layout(constant_id = 1) const bool do_timewarp = false;
layout(constant_id = 2) const bool do_color_correction = true;
layout(constant_id = 3) const int COMP_MAX_LAYERS = 16;
layout(constant_id = 4) const int COMP_VIEWS_PER_LAYER = 2;
layout(constant_id = 5) const int SAMPLER_ARRAY_SIZE = 16;

layout(local_size_x = 8, local_size_y = 8, local_size_z = 1) in;

// layer 0 left color, layer 0 right color, [optional: layer 0 left depth, layer 0 right depth], layer 1 left, layer 1 right, ...
layout(set = 0, binding = 0) uniform sampler2D source[SAMPLER_ARRAY_SIZE];
layout(set = 0, binding = 2) uniform writeonly restrict image2D target;
layout(set = 0, binding = 3, std140) uniform restrict Config
{
	ivec4 views[2];
	vec4 pre_transform[2];
	vec4 post_transform[COMP_MAX_LAYERS][2];

	// corresponds to enum xrt_layer_type
	uvec2 layer_type_and_unpremultiplied[COMP_MAX_LAYERS];

	// which image/sampler(s) correspond to each layer
	ivec2 images_samplers[COMP_MAX_LAYERS][2];

	// for projection layers

	// timewarp matrices
	mat4 transform[COMP_MAX_LAYERS][2];


	// for quad layers

	// all quad transforms and coordinates are in view space
	vec4 quad_position[COMP_MAX_LAYERS][2];
	vec4 quad_normal[COMP_MAX_LAYERS][2];
	mat4 inverse_quad_transform[COMP_MAX_LAYERS][2];

	// quad extent in world scale
	vec2 quad_extent[COMP_MAX_LAYERS];
} ubo;


vec2 position_to_view_uv(ivec2 extent, uint ix, uint iy)
{
	// Turn the index into floating point.
	vec2 xy = vec2(float(ix), float(iy));

	// The inverse of the extent of a view image is the pixel size in [0 .. 1] space.
	vec2 extent_pixel_size = vec2(1.0 / float(extent.x), 1.0 / float(extent.y));

	// Per-target pixel we move the size of the pixels.
	vec2 view_uv = xy * extent_pixel_size;

	// Emulate a triangle sample position by offset half target pixel size.
	view_uv = view_uv + extent_pixel_size / 2.0;

	return view_uv;
}

vec2 transform_uv_subimage(vec2 uv, uint iz, uint layer)
{
	vec2 values = uv;

	// To deal with OpenGL flip and sub image view.
	values.xy = values.xy * ubo.post_transform[layer][iz].zw + ubo.post_transform[layer][iz].xy;

	// Ready to be used.
	return values.xy;
}

vec2 transform_uv_timewarp(vec2 uv, uint view_index, uint layer)
{
	vec4 values = vec4(uv, -1, 1);

	// From uv to tan angle (tangent space).
	values.xy = values.xy * ubo.pre_transform[view_index].zw + ubo.pre_transform[view_index].xy;
	values.y = -values.y; // Flip to OpenXR coordinate system.

	// Timewarp.
	values = ubo.transform[layer][view_index] * values;
	values.xy = values.xy * (1.0 / max(values.w, 0.00001));

	// From [-1, 1] to [0, 1]
	values.xy = values.xy * 0.5 + 0.5;

	// To deal with OpenGL flip and sub image view.
	values.xy = values.xy * ubo.post_transform[layer][view_index].zw + ubo.post_transform[layer][view_index].xy;

	// Done.
	return values.xy;
}

vec2 transform_uv(vec2 uv, uint view_index, uint layer)
{
	if (do_timewarp) {
		return transform_uv_timewarp(uv, view_index, layer);
	} else {
		return transform_uv_subimage(uv, view_index, layer);
	}
}

vec4 do_projection(uint view_index, vec2 view_uv, uint layer)
{
	uint source_image_index = ubo.images_samplers[layer][view_index].x;

	// Do any transformation needed.
	vec2 uv = transform_uv(view_uv, view_index, layer);

	// Sample the source.
	vec4 colour = vec4(texture(source[source_image_index], uv).rgba);

	return colour;
}

vec3 get_direction(vec2 uv, uint view_index)
{
	// Skip the DIM/STRETCH/OFFSET stuff and go directly to values
	vec4 values = vec4(uv, -1, 1);

	// From uv to tan angle (tangent space).
	values.xy = values.xy * ubo.pre_transform[view_index].zw + ubo.pre_transform[view_index].xy;
	values.y = -values.y; // Flip to OpenXR coordinate system.

	vec3 direction = normalize(values.xyz);
	return direction;
}

vec4 do_quad(uint view_index, vec2 view_uv, uint layer)
{
	uint source_image_index = ubo.images_samplers[layer][view_index].x;

	// center point of the plane in view space.
	vec3 quad_position = ubo.quad_position[layer][view_index].xyz;

	// normal vector of the plane.
	vec3 normal = ubo.quad_normal[layer][view_index].xyz;
	normal = normalize(normal);

	// coordinate system is the view space, therefore the camera/eye position is in the origin.
	vec3 camera = vec3(0.0, 0.0, 0.0);

	// default color white should never be visible
	vec4 colour = vec4(1.0, 1.0, 1.0, 1.0);

	//! @todo can we get better "pixel stuck" on projection layers with timewarp uv?
	// never use the timewarp uv here because it depends on the projection layer pose
	vec2 uv = view_uv;

	/*
	* To fill in the view_uv texel on the target texture, an imaginary ray is shot through texels on the target
	* texture. When this imaginary ray hits a quad layer, it means that when the respective color at the hit
	* intersection is picked for the current view_uv texel, the final image as seen through the headset will
	* show this view_uv texel at the respective location.
	*/
	vec3 direction = get_direction(uv, view_index);
	direction = normalize(direction);

	float denominator = dot(direction, normal);

	// denominator is negative when vectors point towards each other, 0 when perpendicular,
	// and positive when vectors point in a similar direction, i.e. direction vector faces quad backface, which we don't render.
	if (denominator < 0.00001) {
		// shortest distance between origin and plane defined by normal + quad_position
		float dist = dot(camera - quad_position, normal);

		// distance between origin and intersection point on the plane.
		float intersection_dist = (dot(camera, normal) + dist) / -denominator;

		// layer is behind camera as defined by direction vector
		if (intersection_dist < 0) {
			colour = vec4(0.0, 0.0, 0.0, 0.0);
			return colour;
		}

		vec3 intersection = camera + intersection_dist * direction;

		// ps for "plane space"
		vec2 intersection_ps = (ubo.inverse_quad_transform[layer][view_index] * vec4(intersection.xyz, 1.0)).xy;

		bool in_plane_bounds =
			intersection_ps.x >= - ubo.quad_extent[layer].x / 2. && //
			intersection_ps.x <= ubo.quad_extent[layer].x / 2. && //
			intersection_ps.y >= - ubo.quad_extent[layer].y / 2. && //
			intersection_ps.y <= ubo.quad_extent[layer].y / 2.;

		if (in_plane_bounds) {
			// intersection_ps is in [-quad_extent .. quad_extent]. Transform to  [0 .. quad_extent], then scale to [ 0 .. 1 ] for sampling
			vec2 plane_uv = (intersection_ps.xy + ubo.quad_extent[layer] / 2.) / ubo.quad_extent[layer];

			// sample on the desired subimage, not the entire texture
			plane_uv = plane_uv * ubo.post_transform[layer][view_index].zw + ubo.post_transform[layer][view_index].xy;

			colour = texture(source[source_image_index], plane_uv);
		} else {
			// intersection on infinite plane outside of plane bounds
			colour = vec4(0.0, 0.0, 0.0, 0.0);
			return colour;
		}
	} else {
		// no intersection with front face of infinite plane or perpendicular
		colour = vec4(0.0, 0.0, 0.0, 0.0);
		return colour;
	}

	return vec4(colour);
}

vec4 do_layers(vec2 view_uv, uint view_index)
{
	vec4 accum = vec4(0, 0, 0, 0);
	for (uint layer = 0; layer < COMP_MAX_LAYERS; layer++) {
		bool use_layer = false;

		vec4 rgba = vec4(0, 0, 0, 0);
		switch (ubo.layer_type_and_unpremultiplied[layer].x) {
			case XRT_LAYER_STEREO_PROJECTION:
			case XRT_LAYER_STEREO_PROJECTION_DEPTH:
				rgba = do_projection(view_index, view_uv, layer);
				use_layer = true;
				break;
			case XRT_LAYER_QUAD:
				rgba = do_quad(view_index, view_uv, layer);
				use_layer = true;
				break;
			default: break;
			}

		if (use_layer) {
			if (ubo.layer_type_and_unpremultiplied[layer].y != 0) {
				// Unpremultipled blend factor of src.a.
				accum.rgb = mix(accum.rgb, rgba.rgb, rgba.a);
			} else {
				// Premultiplied blend factor of 1.
				accum.rgb = (accum.rgb * (1 - rgba.a)) + rgba.rgb;
			}
			accum.a = rgba.a + accum.a * (1.0 - rgba.a);
		}
	}
	return accum;
}

void main()
{
	uint ix = gl_GlobalInvocationID.x;
	uint iy = gl_GlobalInvocationID.y;
	uint iz = gl_GlobalInvocationID.z;

	ivec2 offset = ivec2(ubo.views[iz].xy);
	ivec2 extent = ivec2(ubo.views[iz].zw);

	if (ix >= extent.x || iy >= extent.y) {
		return;
	}

	vec2 view_uv = position_to_view_uv(extent, ix, iy);

	vec4 colour = do_layers(view_uv, iz);

	if (do_color_correction) {
		// Do colour correction here since there are no automatic conversion in hardware available.
		colour.rgb = from_linear_to_srgb(colour.rgb);
	}

	imageStore(target, ivec2(offset.x + ix, offset.y + iy), colour);
}
